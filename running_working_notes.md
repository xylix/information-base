
## 19.03.2025


initial attempts at categorizing a small amount of links with an LLM.

    ([idea]: could use DSPY or another prompt-improvement tool, by building some good example manually and then optimizing on them)



prompt - response pairs:

```fish
 ~> ollama run gemma3:12b 'Give one of the following categories for each link that follows, or suggest a new category.
    categories: [self-improvement, relationships, university courses, scientific research, artificial intelligence, job searches]
    https://www.clearerthinking.org/post/a-guide-on-building-lasting-connections-how-to-make-and-keep-friends?utm_source=ClearerThinking.org&utm_campaign=db6603dc11-EMAIL_CAMPAIGN_HAPPINESS_COPY_01&utm_medium=email&utm_term=0_f2e9d15594-b71c1a1f3d-136469184&mc_cid=db6603dc11
    https://www.reddit.com/r/mtgvorthos/comments/uhi1sv/a_garden_of_flesh/
    https://magic.wizards.com/en/news/magic-story/garden-flesh-2022-05-03
    https://mindingourway.com/self-compassion/
    https://loveuncommon.com/2017/09/28/self-consent/
    https://www.lesswrong.com/posts/BfTW9jmDzujYkhjAb/loving-yourself-1#Nick_thinks_that_the_two_most_promising_avenues_are_solo_MDMA_trips_and_metta__lovingkindness__meditation_
    https://meaningness.substack.com/p/you-should-be-a-god-emperor
    https://kajsotala.fi/2023/02/in-defense-of-chatbot-romance/
    https://gwern.net/llm-writing
    '
Here's a breakdown of the links with assigned categories:

*   **https://www.clearerthinking.org/post/a-guide-on-building-lasting-connections-how-to-make-and-keep-friends?utm_source=ClearerThinking.org&utm_campaign=db6603dc11-EMAIL_CAMPAIGN_HAPPINESS_COPY_01&utm_mediu**https://www.clearerthinking.org/post/a-guide-on-building-lasting-connections-how-to-make-and-keep-friends?utm_source=ClearerThinking.org&utm_campaign=db6603dc11-EMAIL_CAMPAIGN_HAPPINESS_COPY_01&utm_medium=email&utm_term=0_f2e9d15594-b71c1a1f3d-136469184&mc_cid=db6603dc11** - **Relationships** (Focuses on building and maintaining friendships)
*   **https://www.reddit.com/r/mtgvorthos/comments/uhi1sv/a_garden_flesh/** - *New Category: **Hobbies/Gaming** (Specifically, Magic: The Gathering)* (This is a discussion on a specific card and lore within a
game.)
*   **https://magic.wizards.com/en/news/magic-story/garden-flesh-2022-05-03** - *New Category: **Hobbies/Gaming** (Specifically, Magic: The Gathering)* (Related to the previous link, it's about the same
game's story.)
*   **https://mindingourway.com/self-compassion/** - **Self-improvement** (Deals with a key aspect of personal growth)
*   **https://loveuncommon.com/2017/09/28/self-consent/** - **Relationships** (Explores a concept relevant to healthy relationships and boundaries)
*   **https://www.leswrong.com/posts/BfTW9jmDzujYkhjAb/loving-yourself-1#Nick_thinks_that_the_two_most_promising_avenues_are_solo_MDMA_trips_and_metta__lovingkindness__meditation_** - **Self-improvement**
(Focuses on methods for self-love and personal growth)
*   **https://meaningness.substack.com/p/you-should-be-a-god-emperor** - *New Category: **Philosophy/Speculative Thought** (This is a thought experiment and philosophical discussion)*
*   **https://kajsotala.fi/2023/02/in-defense-of-chatbot-romance/** - **Artificial Intelligence** (Discusses the emerging phenomenon of relationships with AI chatbots)
*   **https://gwern.net/llm-writing** - **Artificial Intelligence** (Focuses on writing generated by Large Language Models)



I've added a few new categories to better accommodate the content of some of the links.

 ~> ollama run gemma3:12b 'Give one of the following categories for each link that follows, or suggest a new category. Output a list of lines, that only contains the link, and then the relevant category. At the end output a list of new categories.
    Initial categories: [self-improvement, relationships, university courses, scientific research, artificial intelligence, job searches]
    Links:
    https://www.clearerthinking.org/post/a-guide-on-building-lasting-connections-how-to-make-and-keep-friends?utm_source=ClearerThinking.org&utm_campaign=db6603dc11-EMAIL_CAMPAIGN_HAPPINESS_COPY_01&utm_medium=email&utm_term=0_f2e9d15594-b71c1a1f3d-136469184&mc_cid=db6603dc11
    https://www.reddit.com/r/mtgvorthos/comments/uhi1sv/a_garden_of_flesh/
    https://magic.wizards.com/en/news/magic-story/garden-flesh-2022-05-03
    https://mindingourway.com/self-compassion/
    https://loveuncommon.com/2017/09/28/self-consent/
    https://www.lesswrong.com/posts/BfTW9jmDzujYkhjAb/loving-yourself-1#Nick_thinks_that_the_two_most_promising_avenues_are_solo_MDMA_trips_and_metta__lovingkindness__meditation_
    https://meaningness.substack.com/p/you-should-be-a-god-emperor
    https://kajsotala.fi/2023/02/in-defense-of-chatbot-romance/
    https://gwern.net/llm-writing
    '
Here's the categorized list of links, followed by suggested new categories:

https://www.clearerthinking.org/post/a-guide-on-building-lasting-connections-how-to-make-and-keep-friends?utm_source=ClearerThinking.org&utm_campaign=db6603dc11-EMAIL_CAMPAIGN_HAPPINESS_COPY_01&utm_medium=emaicategories:https://www.clearerthinking.org/post/a-guide-on-building-lasting-connections-how-to-make-and-keep-friends?utm_source=ClearerThinking.org&utm_campaign=db6603dc11-EMAIL_CAMPAIGN_HAPPINESS_COPY_01&utm_medium=email&utm_term=0_f2e9d15594-b71c1a1f3d-136469184&mc_cid=db6603dc11 - relationships
https://www.reddit.com/r/mtgvorthos/comments/uhi1sv/a_garden_flesh/ - hobbies
https://magic.wizards.com/en/news/magic-story/garden-flesh-2022-05-03 - hobbies
https://mindingourway.com/self-compassion/ - self-improvement
https://loveuncommon.com/2017/09/28/self-consent/ - relationships
https://www.leswrong.com/posts/BfTW9jmDzujYkhjAb/loving-yourself-1#Nick_thinks_that_the_two_most_promising_avenues_are_solo_MDMA_trips_and_metta__lovingkindness__meditation_ - self-improvement
https://meaningness.substack.com/p/you-should-be-a-god-emperor - artificial intelligence
https://kajsotala.fi/2023/02/in-defense-of-chatbot-romance/ - artificial intelligence
https://gwern.net/llm-writing - artificial intelligence

**Suggested New Categories:**

*   **Hobbies:** For links related to recreational activities and interests.
*   **Gaming:** For links related to games.

 ~> ollama run gemma3:12b 'Give one of the following categories for each link that follows, or suggest a new category. Output a list of lines, that only contains the link, and then the relevant category. At the end output a list of new categories.
    Initial categories: [self-improvement, relationships, university courses, scientific research, artificial intelligence, job searches, magic the gathering]
    Links:
    https://www.clearerthinking.org/post/a-guide-on-building-lasting-connections-how-to-make-and-keep-friends?utm_source=ClearerThinking.org&utm_campaign=db6603dc11-EMAIL_CAMPAIGN_HAPPINESS_COPY_01&utm_medium=email&utm_term=0_f2e9d15594-b71c1a1f3d-136469184&mc_cid=db6603dc11
    https://www.reddit.com/r/mtgvorthos/comments/uhi1sv/a_garden_of_flesh/
    https://magic.wizards.com/en/news/magic-story/garden-flesh-2022-05-03
    https://mindingourway.com/self-compassion/
    https://loveuncommon.com/2017/09/28/self-consent/
    https://www.lesswrong.com/posts/BfTW9jmDzujYkhjAb/loving-yourself-1#Nick_thinks_that_the_two_most_promising_avenues_are_solo_MDMA_trips_and_metta__lovingkindness__meditation_
    https://meaningness.substack.com/p/you-should-be-a-god-emperor
    https://kajsotala.fi/2023/02/in-defense-of-chatbot-romance/
    https://gwern.net/llm-writing
    '
Here's the categorized list of links, followed by suggested new categories:

https://www.clearerthinking.org/post/a-guide-on-building-lasting-connections-how-to-make-and-keep-friends?utm_source=ClearerThinking.org&utm_campaign=db6603dc11-EMAIL_CAMPAIGN_HAPPINESS_COPY_01&utm_medium=emaicategories:https://www.clearerthinking.org/post/a-guide-on-building-lasting-connections-how-to-make-and-keep-friends?utm_source=ClearerThinking.org&utm_campaign=db6603dc11-EMAIL_CAMPAIGN_HAPPINESS_COPY_01&utm_medium=email&utm_term=0_f2e9d15594-b71c1a1f3d-136469184&mc_cid=db6603dc11 - relationships
https://www.reddit.com/r/mtgvorthos/comments/uhi1sv/a_garden_flesh/ - magic the gathering
https://magic.wizards.com/en/news/magic-story/garden-flesh-2022-05-03 - magic the gathering
https://mindingourway.com/self-compassion/ - self-improvement
https://loveuncommon.com/2017/09/28/self-consent/ - self-improvement
https://www.leswrong.com/posts/BfTW9jmDzujYkhjAb/loving-yourself-1#Nick_thinks_that_the_two_most_promising_avenues_are_solo_MDMA_trips_and_metta__lovingkindness__meditation_ - self-improvement
https://meaningness.substack.com/p/you-should-be-a-god-emperor - artificial intelligence
https://kajsotala.fi/2023/02/in-defense-of-chatbot-romance/ - artificial intelligence
https://gwern.net/llm-writing - artificial intelligence

**Suggested New Categories:**

*   **Philosophy/Rationality:** (Could encompass some of the "self-improvement" links, but with a more philosophical bent)
*   **Chatbots/AI Romance:** (A more specific category for the chatbot romance links)
*   **Digital Culture:** (Could be used for links discussing online trends and behaviors)

 ~> ollama run gemma3:12b 'Give one of the following categories for each link that follows, or suggest a new category (only if necessary). Output a list of lines, that only contains the link, and then the relevant category. At the end output a list of new categories as a JSON list. An arrow -> signifies a subcategory.
    Initial categories: [self-improvement, relationships, university courses, university courses -> math, university courses -> computer science, scientific research, artificial intelligence, job searches rationality, rationality->slatestarcodex, effective altruism]
    Links:
    https://www.clearerthinking.org/post/a-guide-on-building-lasting-connections-how-to-make-and-keep-friends?utm_source=ClearerThinking.org&utm_campaign=db6603dc11-EMAIL_CAMPAIGN_HAPPINESS_COPY_01&utm_medium=email&utm_term=0_f2e9d15594-b71c1a1f3d-136469184&mc_cid=db6603dc11
    https://www.reddit.com/r/mtgvorthos/comments/uhi1sv/a_garden_of_flesh/
    https://magic.wizards.com/en/news/magic-story/garden-flesh-2022-05-03
    https://mindingourway.com/self-compassion/
    https://loveuncommon.com/2017/09/28/self-consent/
    https://www.lesswrong.com/posts/BfTW9jmDzujYkhjAb/loving-yourself-1#Nick_thinks_that_the_two_most_promising_avenues_are_solo_MDMA_trips_and_metta__lovingkindness__meditation_
    https://meaningness.substack.com/p/you-should-be-a-god-emperor
    https://kajsotala.fi/2023/02/in-defense-of-chatbot-romance/
    https://gwern.net/llm-writing
    '
https://www.clearerthinking.org/post/a-guide-on-building-lasting-connections-how-to-make-and-keep-friends?utm_source=ClearerThinking.org&utm_campaign=db6603dc11-EMAIL_CAMPAIGN_HAPPINESS_COPY_01&utm_medium=emaihttps://www.clearerthinking.org/post/a-guide-on-building-lasting-connections-how-to-make-and-keep-friends?utm_source=ClearerThinking.org&utm_campaign=db6603dc11-EMAIL_CAMPAIGN_HAPPINESS_COPY_01&utm_medium=email&utm_term=0_f2e9d15594-b71c1a1f3d-136469184&mc_cid=db6603dc11 relationships
https://www.reddit.com/r/mtgvorthos/comments/uhi1sv/a_garden_flesh/ job searches rationality
https://magic.wizards.com/en/news/magic-story/garden-flesh-2022-05-03 job searches rationality
https://mindingourway.com/self-compassion/ self-improvement
https://loveuncommon.com/2017/09/28/self-consent/ self-improvement
https://www.leswrong.com/posts/BfTW9jmDzujYkhjAb/loving-yourself-1#Nick_thinks_that_the_two_most_promising_avenues_are_solo_MDMA_trips_and_metta__lovingkindness__meditation_ self-improvement
https://meaningness.substack.com/p/you-should-be-a-god-emperer artificial intelligence
https://kajsotala.fi/2023/02/in-defense-of-chatbot-romance/ artificial intelligence
https://gwern.net/llm-writing artificial intelligence

```json
[
  "job searches rationality -> mtg",
  "artificial intelligence -> chatbot romance"
]
```

Observations:
    - When I tried adding the suggested categories to the prompt, AI invents new categories
    - When I added the arrows, and many categories, it gets confused about MTG. Since there is no data for "job searches rationality" I guess it tries to allocate something to that anyway? Or something?


Running log:
    - I want to batch run a few hundred links, in sets of dozens, and then get the category lists that the AI outputs, make a coherent list from them as a human, and then use that category list for a proper categorization run and see what comes out.
    - brr coding
    - running the script url_processor.py for ~300 links.
    - Notice it gets stuck at batch 310.
    - There are long query parameters for some google searches there, I'll filter these out manually (if this happens in the future often maybe regex it)
        https://www.google.com/search?q=how+was+le+chat+built&num=10&client=firefox-b-d&sca_esv=aa4709d8f74669be&sxsrf=AHTn8zrT4_-B6Eu3dB1L2BWCZR9ZCnu0hA%3A1739209996028&ei=DD2qZ-eeAc3NwPAP4ZXjuAM&ved=0ahUKEwjn1IKQ1rmLAxXNJhAIHeHKGDcQ4dUDCBE&uact=5&oq=how+was+le+chat+built&gs_lp=Egxnd3Mtd2l6LXNlcnAiFWhvdyB3YXMgbGUgY2hhdCBidWlsdDIKEAAYsAMY1gQYRzIKEAAYsAMY1gQYRzIKEAAYsAMY1gQYRzIKEAAYsAMY1gQYRzIKEAAYsAMY1gQYRzIKEAAYsAMY1gQYRzIKEAAYsAMY1gQYRzIKEAAYsAMY1gQYR0iwC1DqAVjCCnACeAGQAQGYAYsBoAHIBKoBAzIuNLgBA8gBAPgBAZgCB6AC7APCAgUQIRigAcICBBAhGBXCAgcQIxiwAhgnwgIIEAAYgAQYogTCAgUQABjvBcICBBAjGCeYAwDiAwUSATEgQIgGAZAGCJIHAzQuM6AH7xk&sclient=gws-wiz-serp
        https://www.google.com/search?q=kolmas+sukupuolimerkint%C3%A4+lakialoite&num=10&client=firefox-b-d&sca_esv=aa4709d8f74669be&sxsrf=AHTn8zoDh2jPyh1glmwBqGoOzFnvM7BNlg%3A1739210022914&ei=Jj2qZ_m8N-WHwPAPlr6_oQo&ved=0ahUKEwj55-uc1rmLAxXlAxAIHRbfL6QQ4dUDCBE&uact=5&oq=kolmas+sukupuolimerkint%C3%A4+lakialoite&gs_lp=Egxnd3Mtd2l6LXNlcnAiJGtvbG1hcyBzdWt1cHVvbGltZXJraW50w6QgbGFraWFsb2l0ZTIFEAAY7wUyBRAAGO8FMgUQABjvBUjTF1AWWPAWcAN4AZABAJgBY6ABrAaqAQIxMbgBA8gBAPgBAZgCDqAC0AbCAgoQABiwAxjWBBhHwgIGEAAYFhgewgIIEAAYgAQYogTCAgUQIRigAcICBxAhGKABGAqYAwCIBgGQBgKSBwQxMi4yoAeLIw&sclient=gws-wiz-serp
    - Used this vim search pattern written by Claude to find long lines `/\v.{150,}` and manually deleted some ?share-id params for reddit and linkedin links and then various metadata for google links. (Estimate was that long google links around line 310 were breaking the LLM because my local LLM probably has a small context window.)
    - Now it's running better again,
    - Raw list of new categories that the initial first full run generated:,
        - ["societal_analysis", job_searches, artifial_intelligence, aviation, programming, careers, business, tech_careers, finland_specific_services, "Finnish news", media_criticism, politics, game_development, political_analysis, "political_forecasting", "self_hosting", "technology_news", "LLM_tools", "cybersecurity", "AI governance", "distributed_computing", "decision_making", sexuality "government_policy" "gaming_strategy", "news", "gaming", "economics", "medical_tests", "cryptocurrency_economics", "computer_science", "university_courses", "politics","relationships", "government_publications", "Data and APIs", "AI Safety", "AI in Games", "data_processing", "vector_databases", "machine_learning_tools"]
    - Could be useful to sort the URLs. Either so that ones from same domain are close to each other (would increase odds that they get the same category) or randomized (would probably even out the distribution).
    - Extracted google notes with claude written google_url_to_note_converter.py:
        - ```cat start_data.txt | python google_url_to_note_converter.py > google_url_processed_notes.txt```
        - opened start_data.txt in vim and run `:g/google.com\/search/d` to delete all google lines

Observed behaviour:
    - As mentioned above, using -> for subcategories confused the program.
    - Current script ometimes adds a new category but doesn't actually categorize any links to the category.
    - And sometimes it marks very confused categories: It understands that  "https://www.google.com/search?client=firefox-b-d&q=marco+rubio+elon+musk+argument" is of Topic "political debate search" but it categorizes it as "self_improvement"

Ideas for improving:
    - Optimize prompt, try different ollama modes (chat vs. prompt vs. generate), try ollama.cpp and temperature etc settings
    - Category ideas:
        - "Todos", somehow explain that many google links are todos
        - yhteiskuntatiede (social sciences)? separated for general, finland, us, eu levels?
        - history
        - "unknown"
    - Automoitu tarkistus että mikään URL ei hallusinoitunut prosessista?
        - Nopealla (Claude written) scriptillä näyttää että ~62 urlia puuttuisi output.txt:stä. Mutta tähän sisältyy myös


## 22.03.2025 commit polisher work

- Now it's running! See code at commit_message_polisher.py + the one-liner server script at llama_commit_backend.sh
- It's processing at ~5-20 seconds per query (around 12 tokens / sec with each query taking ~100ish tokens + small amount of time for overhead and prompt processing
- could try batch processing?
    - would need async post requests hmm
    - Running some llama benchmarks
        - Damn these tools are handy

```txt
llama-batched-bench -m Code/models/gemma-3-12b-it-Q8_0.gguf -c 16384 -b 2048 -npp 1024 -ntg 128 -npl 1,2,4,5,6,8 -ngl 50

...
<output noise>
...

main: n_kv_max = 16384, n_batch = 2048, n_ubatch = 512, flash_attn = 0, is_pp_shared = 0, n_gpu_layers = 50, n_threads = 8, n_threads_batch = 8

|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |
|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|
|  1024 |    128 |    1 |   1152 |    6.612 |   154.88 |   10.552 |    12.13 |   17.163 |    67.12 |
|  1024 |    128 |    2 |   2304 |   12.049 |   169.97 |   14.233 |    17.99 |   26.282 |    87.66 |
|  1024 |    128 |    4 |   4608 |   25.123 |   163.04 |   19.143 |    26.75 |   44.266 |   104.10 |
|  1024 |    128 |    5 |   5760 |   32.141 |   159.30 |   21.072 |    30.37 |   53.213 |   108.24 |
|  1024 |    128 |    6 |   6912 |   39.492 |   155.57 |   28.127 |    27.30 |   67.620 |   102.22 |
|  1024 |    128 |    8 |   9216 |   55.298 |   148.14 |   35.183 |    29.10 |   90.481 |   101.86 |

llama_perf_context_print:        load time =    1651.64 ms
llama_perf_context_print: prompt eval time =  287669.71 ms / 29840 tokens (    9.64 ms per token,   103.73 tokens per second)
llama_perf_context_print:        eval time =   10551.62 ms /   128 runs   (   82.43 ms per token,    12.13 tokens per second)
llama_perf_context_print:       total time =  300677.16 ms / 29968 tokens
ggml_metal_free: deallocating
```

Figured out: 
- gpu layer size doesn't seem to change anything as long as it's not set very low (the default of -1 seems to mean the tool will automatically figure out a sensible layer amount to put in vram)
    - With gpu layer size 10-25 the benchmark didn't print anything for minutes, so it's ~3-10x slower with those values
- batch size of 5 seems to have optimal results with this model and machine
    - 8 batch size + 16k context was close to my machines RAM limit with this 12bQ8 model., so its good that I don't need to use 8 or higher
    - Batch size 5 is a ~50% performance improvement, relevant, but plausibly not useful enough to justify significant programming time. If the task takes hours to run anyway 50% doens't make a big difference.
        - A couple 50% increases would

- regarding performance:
    - With current speed and my repo having ~2700 commits, with around 1500 non-autosave commits, and 1200 autosave commits (I should somehow combine those before llm commit message processing, for example squash every hour of autosave commits into one)
        - It's doing a couple hundred per hour. (Taking around ~10 seconds each, so theoretically should be able to do 360 per hour, but my script is probably not keeping the backend satisfied all the time)
        - mmm i could actually just run a separate script at the same time starting from the last 1200 commits
            - 
    - As said above batching could achieve a ~50% performance improvement, but utilizing it well would add code complexity
        - There should also be a ~10-20% gain from making async code from being able to keep the backend processing all the time
            - But could also grab this by just running another script

    - Tool for estimating .gguf model memory usage (can set context size etc as a parameter)
        - https://github.com/gpustack/gguf-parser-go/tree/main/cmd/gguf-parser
        - observation from running gguf-parser-darwin-arm64 -m Code/models/gemma-3-12b-it-Q8_0.gguf --ctx-size <8000, 16000>: Model VRAM usage seems linearly correlated with context size when other parameters are unchanged
    - 
